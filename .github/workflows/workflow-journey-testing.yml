# Workflow to Journey Mapping System - Comprehensive CI/CD Testing Pipeline
# ========================================================================
#
# Automated testing pipeline that validates the complete workflow-to-journey
# conversion system across multiple environments and scenarios.
#
# Test Coverage:
# - Conversion accuracy validation
# - Block type coverage testing
# - Performance and reliability benchmarks
# - End-to-end integration testing
# - Cross-platform compatibility
# - Security and compliance validation

name: 'Workflow Journey Testing Pipeline'

on:
  push:
    branches: [ main, develop, 'feature/workflow-journey-*' ]
    paths:
      - 'apps/sim/services/parlant/**'
      - 'apps/sim/__tests__/**'
      - 'packages/parlant-server/**'
      - '.github/workflows/workflow-journey-testing.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'apps/sim/services/parlant/**'
      - 'apps/sim/__tests__/**'
      - 'packages/parlant-server/**'
      - '.github/workflows/workflow-journey-testing.yml'
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Test Level'
        required: true
        default: 'full'
        type: choice
        options:
          - 'full'
          - 'basic'
          - 'performance'
          - 'integration'
      parallel_jobs:
        description: 'Number of parallel test jobs'
        required: false
        default: '4'
        type: string

env:
  NODE_VERSION: '20'
  BUN_VERSION: '1.2.13'
  PYTHON_VERSION: '3.12'
  TEST_TIMEOUT: 600000  # 10 minutes
  PERFORMANCE_TEST_TIMEOUT: 1800000  # 30 minutes

  # Test configuration
  TEST_LEVEL: ${{ github.event.inputs.test_level || 'full' }}
  PARALLEL_JOBS: ${{ github.event.inputs.parallel_jobs || '4' }}

  # Database and service URLs for testing
  TEST_DATABASE_URL: 'postgresql://test:test@localhost:5432/workflow_journey_test'
  TEST_PARLANT_SERVER_URL: 'http://localhost:8000'
  TEST_SOCKET_SERVER_URL: 'http://localhost:3001'

jobs:
  # =====================================================
  # Setup and Validation Jobs
  # =====================================================

  setup-validation:
    name: 'Environment Setup & Validation'
    runs-on: ubuntu-latest
    timeout-minutes: 10

    outputs:
      test-matrix: ${{ steps.generate-matrix.outputs.matrix }}
      should-run-performance: ${{ steps.check-conditions.outputs.performance }}
      should-run-integration: ${{ steps.check-conditions.outputs.integration }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for change detection

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: ${{ env.BUN_VERSION }}

      - name: Install Dependencies
        run: |
          bun install --frozen-lockfile
          cd packages/parlant-server && python -m pip install -r requirements.txt

      - name: Validate Test Environment
        run: |
          echo "🔍 Validating test environment..."
          bun --version
          node --version
          npm --version
          python3 --version

          # Check for required test files
          echo "📋 Checking test files..."
          ls -la apps/sim/__tests__/

          echo "✅ Environment validation complete"

      - name: Generate Test Matrix
        id: generate-matrix
        run: |
          # Generate dynamic test matrix based on test level and parallel jobs
          matrix=$(node -e "
            const testLevel = '${{ env.TEST_LEVEL }}';
            const parallelJobs = parseInt('${{ env.PARALLEL_JOBS }}');

            let tests = [];

            if (testLevel === 'full' || testLevel === 'basic') {
              tests.push(
                { name: 'conversion-accuracy', path: 'integration/workflow-journey-conversion-accuracy.test.ts', timeout: 10 },
                { name: 'block-validation', path: 'integration/workflow-block-validation.test.ts', timeout: 15 }
              );
            }

            if (testLevel === 'full' || testLevel === 'performance') {
              tests.push(
                { name: 'performance', path: 'performance/workflow-journey-performance.test.ts', timeout: 30 }
              );
            }

            if (testLevel === 'full' || testLevel === 'integration') {
              tests.push(
                { name: 'e2e-integration', path: 'e2e/workflow-to-conversation-integration.test.ts', timeout: 20 }
              );
            }

            console.log(JSON.stringify({ include: tests }));
          ")
          echo "matrix=$matrix" >> $GITHUB_OUTPUT

      - name: Check Test Conditions
        id: check-conditions
        run: |
          # Determine which test suites should run based on conditions
          should_run_performance="true"
          should_run_integration="true"

          # Skip performance tests on PR if not explicitly requested
          if [[ "${{ github.event_name }}" == "pull_request" && "${{ env.TEST_LEVEL }}" != "performance" && "${{ env.TEST_LEVEL }}" != "full" ]]; then
            should_run_performance="false"
          fi

          # Skip integration tests if dependencies are not available
          if [[ ! -f "packages/parlant-server/requirements.txt" ]]; then
            should_run_integration="false"
          fi

          echo "performance=$should_run_performance" >> $GITHUB_OUTPUT
          echo "integration=$should_run_integration" >> $GITHUB_OUTPUT

  # =====================================================
  # Core Testing Jobs
  # =====================================================

  conversion-accuracy-tests:
    name: 'Conversion Accuracy Tests'
    runs-on: ubuntu-latest
    needs: setup-validation
    timeout-minutes: 15

    strategy:
      fail-fast: false
      matrix:
        test-scenario:
          - 'simple-workflows'
          - 'complex-workflows'
          - 'edge-cases'
          - 'parameter-substitution'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Test Environment
        uses: ./.github/actions/setup-test-env

      - name: Run Conversion Accuracy Tests
        run: |
          echo "🧪 Running conversion accuracy tests for: ${{ matrix.test-scenario }}"

          cd apps/sim
          bun test __tests__/integration/workflow-journey-conversion-accuracy.test.ts \
            --reporter=verbose \
            --timeout=${{ env.TEST_TIMEOUT }} \
            --grep="${{ matrix.test-scenario }}"

      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: conversion-accuracy-results-${{ matrix.test-scenario }}
          path: |
            apps/sim/test-results/
            apps/sim/coverage/
          retention-days: 7

  block-validation-tests:
    name: 'Block Validation Tests'
    runs-on: ubuntu-latest
    needs: setup-validation
    timeout-minutes: 20

    strategy:
      fail-fast: false
      matrix:
        block-group:
          - 'core-blocks'      # starter, condition, parallel, router
          - 'ai-blocks'        # agent, function, evaluator
          - 'api-blocks'       # api, webhook
          - 'database-blocks'  # postgresql, mysql, mongodb
          - 'communication'    # gmail, slack, discord, sms
          - 'external-services' # github, jira, google-drive

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Test Environment
        uses: ./.github/actions/setup-test-env

      - name: Run Block Validation Tests
        run: |
          echo "🔧 Running block validation tests for: ${{ matrix.block-group }}"

          cd apps/sim
          bun test __tests__/integration/workflow-block-validation.test.ts \
            --reporter=verbose \
            --timeout=${{ env.TEST_TIMEOUT }} \
            --grep="${{ matrix.block-group }}"

      - name: Generate Block Coverage Report
        run: |
          cd apps/sim
          echo "📊 Generating block coverage report..."

          # Generate coverage report for block types
          bun test:coverage --include="**/*block*.ts" --exclude="**/*.test.ts"

      - name: Upload Block Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: block-validation-results-${{ matrix.block-group }}
          path: |
            apps/sim/test-results/
            apps/sim/coverage/
          retention-days: 7

  # =====================================================
  # Performance Testing Jobs
  # =====================================================

  performance-tests:
    name: 'Performance & Reliability Tests'
    runs-on: ubuntu-latest
    needs: setup-validation
    if: needs.setup-validation.outputs.should-run-performance == 'true'
    timeout-minutes: 45

    strategy:
      fail-fast: false
      matrix:
        performance-scenario:
          - 'conversion-benchmarks'
          - 'memory-usage'
          - 'concurrent-load'
          - 'cache-effectiveness'
          - 'reliability-recovery'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Test Environment
        uses: ./.github/actions/setup-test-env

      - name: Setup Performance Monitoring
        run: |
          # Install additional tools for performance monitoring
          npm install -g clinic autocannon

          # Setup system monitoring
          echo "📊 Setting up performance monitoring..."

      - name: Run Performance Tests
        run: |
          echo "⚡ Running performance tests for: ${{ matrix.performance-scenario }}"

          cd apps/sim

          # Run performance tests with detailed monitoring
          bun test __tests__/performance/workflow-journey-performance.test.ts \
            --reporter=verbose \
            --timeout=${{ env.PERFORMANCE_TEST_TIMEOUT }} \
            --grep="${{ matrix.performance-scenario }}"

      - name: Generate Performance Report
        if: always()
        run: |
          cd apps/sim
          echo "📈 Generating performance report..."

          # Create performance report
          cat > performance-report-${{ matrix.performance-scenario }}.json << EOF
          {
            "scenario": "${{ matrix.performance-scenario }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "environment": {
              "runner": "${{ runner.os }}",
              "node_version": "$(node --version)",
              "memory_total": "$(free -h | awk '/^Mem:/ { print \$2 }')",
              "cpu_cores": "$(nproc)"
            }
          }
          EOF

      - name: Upload Performance Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results-${{ matrix.performance-scenario }}
          path: |
            apps/sim/test-results/
            apps/sim/performance-report-*.json
            apps/sim/coverage/
          retention-days: 14

  # =====================================================
  # Integration Testing Jobs
  # =====================================================

  integration-tests:
    name: 'End-to-End Integration Tests'
    runs-on: ubuntu-latest
    needs: setup-validation
    if: needs.setup-validation.outputs.should-run-integration == 'true'
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: test
          POSTGRES_PASSWORD: test
          POSTGRES_DB: workflow_journey_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    strategy:
      fail-fast: false
      matrix:
        integration-scenario:
          - 'linear-workflows'
          - 'branching-conversations'
          - 'long-running-processes'
          - 'real-time-communication'
          - 'error-handling'
          - 'concurrent-users'

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Test Environment
        uses: ./.github/actions/setup-test-env

      - name: Setup Python Environment
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Start Parlant Server
        run: |
          echo "🚀 Starting Parlant server for integration tests..."

          cd packages/parlant-server
          pip install -r requirements.txt

          # Start Parlant server in background
          python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
          echo $! > parlant_server.pid

          # Wait for server to be ready
          sleep 10
          curl -f ${{ env.TEST_PARLANT_SERVER_URL }}/health || (echo "Parlant server failed to start" && exit 1)

      - name: Start Socket Server
        run: |
          echo "🔌 Starting Socket.io server for real-time tests..."

          cd apps/sim
          bun run dev:sockets &
          echo $! > socket_server.pid

          # Wait for socket server to be ready
          sleep 5

      - name: Run Integration Tests
        run: |
          echo "🔄 Running integration tests for: ${{ matrix.integration-scenario }}"

          cd apps/sim

          # Set test environment variables
          export TEST_DATABASE_URL="${{ env.TEST_DATABASE_URL }}"
          export TEST_PARLANT_SERVER_URL="${{ env.TEST_PARLANT_SERVER_URL }}"
          export TEST_SOCKET_SERVER_URL="${{ env.TEST_SOCKET_SERVER_URL }}"

          bun test __tests__/e2e/workflow-to-conversation-integration.test.ts \
            --reporter=verbose \
            --timeout=${{ env.TEST_TIMEOUT }} \
            --grep="${{ matrix.integration-scenario }}"

      - name: Collect Service Logs
        if: always()
        run: |
          echo "📋 Collecting service logs..."

          # Collect Parlant server logs
          if [[ -f packages/parlant-server/parlant_server.pid ]]; then
            pid=$(cat packages/parlant-server/parlant_server.pid)
            if ps -p $pid > /dev/null; then
              kill $pid
            fi
          fi

          # Collect Socket server logs
          if [[ -f apps/sim/socket_server.pid ]]; then
            pid=$(cat apps/sim/socket_server.pid)
            if ps -p $pid > /dev/null; then
              kill $pid
            fi
          fi

      - name: Upload Integration Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-results-${{ matrix.integration-scenario }}
          path: |
            apps/sim/test-results/
            apps/sim/coverage/
            packages/parlant-server/logs/
          retention-days: 7

  # =====================================================
  # Cross-Platform Testing
  # =====================================================

  cross-platform-tests:
    name: 'Cross-Platform Compatibility'
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        node-version: ['18', '20']

    runs-on: ${{ matrix.os }}
    needs: setup-validation
    timeout-minutes: 20

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Setup Platform-Specific Tools
        shell: bash
        run: |
          if [[ "${{ runner.os }}" == "Windows" ]]; then
            # Windows-specific setup
            choco install python3 -y
          elif [[ "${{ runner.os }}" == "macOS" ]]; then
            # macOS-specific setup
            brew install python@3.12
          fi

      - name: Install Dependencies
        run: npm install --frozen-lockfile

      - name: Run Cross-Platform Tests
        run: |
          echo "🌐 Running cross-platform compatibility tests on ${{ matrix.os }} with Node ${{ matrix.node-version }}"

          cd apps/sim
          npm test -- __tests__/integration/workflow-journey-conversion-accuracy.test.ts \
            --reporter=verbose \
            --timeout=300000

      - name: Upload Cross-Platform Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cross-platform-results-${{ matrix.os }}-node${{ matrix.node-version }}
          path: |
            apps/sim/test-results/
          retention-days: 3

  # =====================================================
  # Security and Compliance Testing
  # =====================================================

  security-tests:
    name: 'Security & Compliance Tests'
    runs-on: ubuntu-latest
    needs: setup-validation
    timeout-minutes: 15

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Setup Test Environment
        uses: ./.github/actions/setup-test-env

      - name: Run Security Audit
        run: |
          echo "🔒 Running security audit..."

          # NPM audit
          npm audit --audit-level=moderate

          # Check for secrets in code
          echo "🔍 Scanning for secrets..."
          npm install -g secret-scan
          secret-scan apps/sim/services/parlant/

      - name: Run SAST Analysis
        run: |
          echo "🔍 Running static application security testing..."

          # Install and run semgrep
          python3 -m pip install semgrep
          semgrep --config=p/security-audit apps/sim/services/parlant/

      - name: Dependency Vulnerability Check
        run: |
          echo "📦 Checking dependencies for vulnerabilities..."

          # Install and run audit tools
          npm install -g audit-ci
          audit-ci --config audit-ci.json

      - name: Upload Security Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-test-results
          path: |
            security-report.json
            audit-results.json
          retention-days: 30

  # =====================================================
  # Results Consolidation and Reporting
  # =====================================================

  consolidate-results:
    name: 'Consolidate Test Results'
    runs-on: ubuntu-latest
    needs: [conversion-accuracy-tests, block-validation-tests, performance-tests, integration-tests, cross-platform-tests, security-tests]
    if: always()
    timeout-minutes: 10

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Download All Test Artifacts
        uses: actions/download-artifact@v4
        with:
          path: test-artifacts/

      - name: Generate Comprehensive Report
        run: |
          echo "📊 Generating comprehensive test report..."

          # Create report directory
          mkdir -p final-report

          # Generate unified test report
          node -e "
            const fs = require('fs');
            const path = require('path');

            const report = {
              timestamp: new Date().toISOString(),
              workflow_id: '${{ github.run_id }}',
              commit_sha: '${{ github.sha }}',
              branch: '${{ github.ref_name }}',
              test_level: '${{ env.TEST_LEVEL }}',
              results: {
                conversion_accuracy: { status: 'unknown', details: [] },
                block_validation: { status: 'unknown', details: [] },
                performance: { status: 'unknown', details: [] },
                integration: { status: 'unknown', details: [] },
                cross_platform: { status: 'unknown', details: [] },
                security: { status: 'unknown', details: [] }
              },
              summary: {
                total_tests: 0,
                passed: 0,
                failed: 0,
                skipped: 0,
                duration: 0
              }
            };

            // Process artifacts and gather results
            const artifactsDir = 'test-artifacts';
            if (fs.existsSync(artifactsDir)) {
              const artifacts = fs.readdirSync(artifactsDir);
              console.log('Processing artifacts:', artifacts);

              // Add logic to parse test results from artifacts
              // This would be expanded based on actual test output formats
            }

            // Write final report
            fs.writeFileSync('final-report/comprehensive-test-report.json', JSON.stringify(report, null, 2));

            console.log('✅ Comprehensive test report generated');
          "

      - name: Generate Badge Data
        run: |
          echo "🏷️ Generating status badges..."

          # Extract summary data for badges
          node -e "
            const report = JSON.parse(require('fs').readFileSync('final-report/comprehensive-test-report.json'));
            const passRate = ((report.summary.passed / report.summary.total_tests) * 100).toFixed(1);

            const badges = {
              tests: {
                schemaVersion: 1,
                label: 'tests',
                message: \`\${report.summary.passed}/\${report.summary.total_tests} passed\`,
                color: passRate >= 90 ? 'brightgreen' : passRate >= 75 ? 'yellow' : 'red'
              },
              conversion: {
                schemaVersion: 1,
                label: 'conversion accuracy',
                message: report.results.conversion_accuracy.status,
                color: report.results.conversion_accuracy.status === 'passed' ? 'brightgreen' : 'red'
              }
            };

            require('fs').writeFileSync('final-report/test-badges.json', JSON.stringify(badges, null, 2));
          "

      - name: Upload Final Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: final-report/
          retention-days: 30

      - name: Comment PR Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('final-report/comprehensive-test-report.json'));

            const passRate = ((report.summary.passed / report.summary.total_tests) * 100).toFixed(1);
            const statusEmoji = passRate >= 90 ? '✅' : passRate >= 75 ? '⚠️' : '❌';

            const comment = \`
            ## ${statusEmoji} Workflow Journey Testing Results

            **Overall Pass Rate: ${passRate}% (${report.summary.passed}/${report.summary.total_tests})**

            ### Test Results Summary
            | Test Suite | Status | Details |
            |------------|--------|---------|
            | Conversion Accuracy | ${report.results.conversion_accuracy.status} | ${report.results.conversion_accuracy.details.length} scenarios |
            | Block Validation | ${report.results.block_validation.status} | All block types tested |
            | Performance | ${report.results.performance.status} | Benchmarks completed |
            | Integration | ${report.results.integration.status} | E2E flows validated |
            | Cross Platform | ${report.results.cross_platform.status} | Multi-OS compatibility |
            | Security | ${report.results.security.status} | Security audit completed |

            **Test Level:** ${report.test_level}
            **Commit:** ${report.commit_sha.substring(0, 8)}
            **Workflow ID:** ${report.workflow_id}

            [View detailed report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            \`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Update Status Check
        if: always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('final-report/comprehensive-test-report.json'));

            const passRate = ((report.summary.passed / report.summary.total_tests) * 100).toFixed(1);
            const state = passRate >= 75 ? 'success' : 'failure';

            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: '${{ github.sha }}',
              state: state,
              context: 'Workflow Journey Testing',
              description: \`Pass rate: \${passRate}% (\${report.summary.passed}/\${report.summary.total_tests})\`,
              target_url: \`https://github.com/\${context.repo.owner}/\${context.repo.repo}/actions/runs/${{ github.run_id }}\`
            });

# =====================================================
# Reusable Workflows and Actions
# =====================================================

# Note: Additional composite actions would be defined in separate files:
# - .github/actions/setup-test-env/action.yml
# - .github/actions/setup-parlant-server/action.yml
# - .github/actions/collect-test-results/action.yml